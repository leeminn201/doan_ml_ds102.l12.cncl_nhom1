{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bản sao của 25.52gb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fppnuDmdDjFr"
      },
      "source": [
        "***Keras***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEMdI1jkP3mX"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Input, Model\r\n",
        "from keras.layers import LSTM, Dense, Embedding, concatenate, Dropout, concatenate\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwLwoeNOrk8"
      },
      "source": [
        "Đọc và Xử Lý Dữ Liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ-7CrjCDiVK"
      },
      "source": [
        "dataset=pd.read_csv('/content/mynew (9).csv',delimiter=',', error_bad_lines=False)\n",
        "dataset=dataset.replace('\\0','')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s8TGjFe2zu2"
      },
      "source": [
        "dataset2=pd.read_excel('/content/Constraint_English_Val.xlsx')\n",
        "dataset2=dataset2.replace('\\0','')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_b61Qi3sZBn"
      },
      "source": [
        "stop_words = []\n",
        "stop_words = pd.read_csv('/content/stop_words.txt', sep='\\n', header=None)[0].tolist()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28bCg1Pd-fWG"
      },
      "source": [
        "def clean_text(string: str, punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',stop_words=stop_words) -> str:\n",
        "    \"\"\"\n",
        "    Làm sạch dữ liệu  \n",
        "    \"\"\"\n",
        "    # Xóa urls\n",
        "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
        "\n",
        "    # Xóa thẻ html\n",
        "    string = re.sub(r'<.*?>', '', string)\n",
        "\n",
        "    # Xóa dấu câu\n",
        "    for x in string.lower(): \n",
        "        if x in punctuations: \n",
        "            string = string.replace(x, \"\") \n",
        "\n",
        "    # Chuyển về kiểu chữ thường\n",
        "    string = string.lower()\n",
        "\n",
        "    # Xóa từ trong stop word\n",
        "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
        "\n",
        "    # Xóa khoảng trắng\n",
        "    string = re.sub(r'\\s+', ' ', string).strip()\n",
        "\n",
        "    return string"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdS1gCgoD4WT"
      },
      "source": [
        "corpus = []\n",
        "for review in dataset.values[:, 1]:\n",
        "    review=clean_text(review)\n",
        "    corpus.append(review)\n",
        "X_train=corpus\n",
        "Y_train = dataset.iloc[:,2:3]\n",
        "labelencoder_z = LabelEncoder()\n",
        "Y_train.iloc[:,0] = labelencoder_z.fit_transform(Y_train.iloc[:,0])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGzxy0K0k7RI"
      },
      "source": [
        "corpus1 = []\n",
        "for review in dataset2.values[:, 1]:\n",
        "    review=clean_text(review)\n",
        "    corpus1.append(review)\n",
        "X_test=corpus1\n",
        "Y_test = dataset2.iloc[:,2:3]\n",
        "labelencoder_z = LabelEncoder()\n",
        "Y_test.iloc[:,0] = labelencoder_z.fit_transform(Y_test.iloc[:,0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPyt2ngeAjCm"
      },
      "source": [
        "class Embeddings():\n",
        "    \"\"\"\n",
        "    Lớp đọc file word embedding và tạo ma trận dựa trên file đó\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, vector_dimension):\n",
        "        self.path = path \n",
        "        self.vector_dimension = vector_dimension\n",
        "    \n",
        "    @staticmethod\n",
        "    # Chuyển đầu vào thành một mảng\n",
        "    def get_coefs(word, *arr): \n",
        "        return word, np.asarray(arr, dtype='float32')\n",
        "    # Sử dụng dụng file word embedding có sẵn tạo thành từ điển\n",
        "    def get_embedding_index(self):\n",
        "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
        "        return embeddings_index\n",
        "    # Tạo ma trận\n",
        "    def create_embedding_matrix(self, tokenizer, max_features):\n",
        "        \"\"\"\n",
        "        Hàm tạo ma trận embedding\n",
        "        \"\"\"\n",
        "        model_embed = self.get_embedding_index()\n",
        "\n",
        "        embedding_matrix = np.zeros((max_features + 1, self.vector_dimension))\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index > max_features:\n",
        "                break\n",
        "            else:\n",
        "                try:\n",
        "                    embedding_matrix[index] = model_embed[word]\n",
        "                except:\n",
        "                    continue\n",
        "        return embedding_matrix"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTcwHtGnFutc"
      },
      "source": [
        "\n",
        "class TextToTensor():\n",
        "    \"\"\" \n",
        "    Lớp chuyển đổi từ text sang số và tạo thành ma trận\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def string_to_tensor(self, string_list: list) -> list:\n",
        "        \"\"\"\n",
        "        Hàm chuyển đổi từ text sang vecto\n",
        "        \"\"\"    \n",
        "        string_list = self.tokenizer.texts_to_sequences(string_list)\n",
        "        string_list = pad_sequences(string_list, maxlen=self.max_len)\n",
        "        \n",
        "        return string_list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyQRbY2MIOjb"
      },
      "source": [
        "class RnnModel():\n",
        "    \"\"\"\n",
        "    RNN để phân tích ngữ nghĩa\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_matrix, embedding_dim, max_len, X_additional=None):\n",
        "        \n",
        "        inp1 = Input(shape=(max_len,))\n",
        "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
        "        x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "        x = Bidirectional(LSTM(150))(x)\n",
        "        x = Dense(128, activation=\"relu\")(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(64, activation=\"relu\")(x)\n",
        "        x = Dense(1, activation=\"sigmoid\")(x)    \n",
        "        model = Model(inputs=inp1, outputs=x)\n",
        "\n",
        "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
        "        self.model = model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CctYnOYkIksJ"
      },
      "source": [
        "\n",
        "class Pipeline:\n",
        "    \"\"\"\n",
        "    Class Pipeline\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        X_train: list, \n",
        "        Y_train: list, \n",
        "        embed_path: str, \n",
        "        embed_dim: int,\n",
        "        stop_words=[],\n",
        "        X_test=[], \n",
        "        Y_test=[],\n",
        "        max_len=None,\n",
        "        epochs=3,\n",
        "        batch_size=256\n",
        "        ):\n",
        "\n",
        "        # Tiền Xử Lý Văn Bản\n",
        "        X_train = [clean_text(text, stop_words=stop_words) for text in X_train]\n",
        "        Y_train = np.asarray(Y_train)\n",
        "        \n",
        "        # Mã hóa văn bản\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "        # Lưu tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Tạo Ma Trận Embeddings\n",
        "        embedding = Embeddings(embed_path, embed_dim)\n",
        "        embedding_matrix = embedding.create_embedding_matrix(tokenizer, len(tokenizer.word_counts))\n",
        "\n",
        "        # Tạo đầu vào cho mô hình\n",
        "        if max_len is None:\n",
        "            max_len = np.max([len(text.split()) for text in X_train])\n",
        "        TextToTensor_instance = TextToTensor(\n",
        "            tokenizer=tokenizer, \n",
        "            max_len=max_len\n",
        "            )\n",
        "        X_train = TextToTensor_instance.string_to_tensor(X_train)\n",
        "\n",
        "        # Tạo Model\n",
        "        rnn = RnnModel(\n",
        "            embedding_matrix=embedding_matrix, \n",
        "            embedding_dim=embed_dim, \n",
        "            max_len=max_len\n",
        "        )\n",
        "        rnn.model.fit(\n",
        "            X_train,\n",
        "            Y_train, \n",
        "            batch_size=batch_size, \n",
        "            epochs=epochs\n",
        "        )\n",
        "\n",
        "        self.model = rnn.model\n",
        "\n",
        "        # Nếu có X_test sẽ đưa ra dự đoán\n",
        "        if len(X_test)>0:\n",
        "            X_test = [clean_text(text) for text in X_test]\n",
        "            X_test = TextToTensor_instance.string_to_tensor(X_test)\n",
        "            yhat = [x[0] for x in rnn.model.predict(X_test).tolist()]\n",
        "            \n",
        "            self.yhat = yhat\n",
        "\n",
        "            # So sánh kết quả dự đoán và thực tế\n",
        "            if len(Y_test)>0:\n",
        "                self.acc = accuracy_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])\n",
        "                self.f1 = f1_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnvzlXLwtHIf"
      },
      "source": [
        "import yaml\n",
        "with open(\"conf.yml\", 'r') as file:\n",
        "    conf = yaml.safe_load(file).get('pipeline')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIViucGIwIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ea3591-5222-49df-cf2b-b9246f0cf083"
      },
      "source": [
        "results = Pipeline(\n",
        "    X_train=X_train,\n",
        "    Y_train=Y_train, \n",
        "    embed_path='/content/glove.42B.300d.txt',\n",
        "    embed_dim=300,\n",
        "    stop_words=stop_words,\n",
        "    X_test=X_test,\n",
        "    Y_test=Y_test,\n",
        "    max_len=conf.get('max_len'),\n",
        "    epochs=conf.get('epochs'),\n",
        "    batch_size=conf.get('batch_size')\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "26/26 [==============================] - 15s 579ms/step - loss: 0.3773\n",
            "Epoch 2/7\n",
            "26/26 [==============================] - 15s 577ms/step - loss: 0.1810\n",
            "Epoch 3/7\n",
            "26/26 [==============================] - 15s 576ms/step - loss: 0.1035\n",
            "Epoch 4/7\n",
            "26/26 [==============================] - 15s 577ms/step - loss: 0.0498\n",
            "Epoch 5/7\n",
            "26/26 [==============================] - 15s 580ms/step - loss: 0.0223\n",
            "Epoch 6/7\n",
            "26/26 [==============================] - 15s 579ms/step - loss: 0.0176\n",
            "Epoch 7/7\n",
            "26/26 [==============================] - 15s 575ms/step - loss: 0.0078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-s5M--yfi8S",
        "outputId": "09624f91-a1b0-47dd-96cf-64eb7408f6ea"
      },
      "source": [
        "print(results.acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.922429906542056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENPctplm2MJ1"
      },
      "source": [
        "Y_pred=[1 if x > 0.5 else 0 for x in results.yhat]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbNtCGVJ1nau",
        "outputId": "00f57fe9-2ddb-4255-cb4c-28d7f78d5eed"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(Y_test,Y_pred,digits=5))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.91618   0.92157   0.91887      1020\n",
            "           1    0.92819   0.92321   0.92569      1120\n",
            "\n",
            "    accuracy                        0.92243      2140\n",
            "   macro avg    0.92218   0.92239   0.92228      2140\n",
            "weighted avg    0.92246   0.92243   0.92244      2140\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5UydDmZuc_N"
      },
      "source": [
        "\n",
        "test1  = [ \"ngồi một mình trong đêm\" ]\n",
        "test2  = [ \"what is covid\" ]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpK6BqscJLc4"
      },
      "source": [
        "TextToTensor_instance = TextToTensor(tokenizer=results.tokenizer,max_len=20)\n",
        "\n",
        "test1_nn = TextToTensor_instance.string_to_tensor(test1)\n",
        "test2_nn = TextToTensor_instance.string_to_tensor(test2)\n",
        "\n",
        "p_test1 = results.model.predict(test1_nn)[0][0]\n",
        "p_test2 = results.model.predict(test2_nn)[0][0]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRkfu-lZu37l",
        "outputId": "3f9de555-d91d-4ee4-e42c-98afd4055454"
      },
      "source": [
        "print(f'Sentence: {p_test1} Score: {p_test1}')\n",
        "print(f'Sentence: {p_test2} Score: {p_test2}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: 0.026009470224380493 Score: 0.026009470224380493\n",
            "Sentence: 0.539668619632721 Score: 0.539668619632721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga7If8XdvaMF"
      },
      "source": [
        "# Saving the predictions\n",
        "test['prob_is_genuine'] = results.yhat\n",
        "test['target'] = [1 if x > 0.5 else 0 for x in results.yhat]\n",
        " \n",
        "# Saving the predictions to a csv file\n",
        "if conf.get('save_results'):\n",
        "    if not os.path.isdir('output'):\n",
        "        os.mkdir('output')    \n",
        "    test[['id', 'target']].to_csv(f'output/submission_{date.today()}.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tf1K8sQKkHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4fb59d3-c5ba-44f3-c98a-4e46dca5a5a8"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-14 17:08:37--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2020-12-14 17:08:37--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2020-12-14 17:08:38--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  2.14MB/s    in 14m 32s \n",
            "\n",
            "2020-12-14 17:23:10 (2.05 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SzgYw3APUih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c27fe5b-1bf4-420c-d764-568a24db3cba"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}